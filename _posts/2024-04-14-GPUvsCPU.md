# The competition base on bird 

- CPU

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.076692</td>
      <td>0.025803</td>
      <td>0.005155</td>
      <td>00:54</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.037787</td>
      <td>0.020217</td>
      <td>0.010309</td>
      <td>00:55</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.020219</td>
      <td>0.012819</td>
      <td>0.007732</td>
      <td>00:57</td>
    </tr>
  </tbody>
</table>

## GPU with 45% util
default batch size is 64

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.050168</td>
      <td>0.032649</td>
      <td>0.013769</td>
      <td>00:06</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.032566</td>
      <td>0.017594</td>
      <td>0.005164</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.018081</td>
      <td>0.010844</td>
      <td>0.005164</td>
      <td>00:05</td>
    </tr>
  </tbody>
</table>


![nvtop](/images/2024-04-14-GPUvsCPU/image.png)


### change batch 

step1, enlarge shm size to 8g.\
Still take 5 second every epoch.\

change batch to 16, 32, 64, 128, and 256.\
using command `dls.bs=16`

They all take around 5 second every epoch. So in this case bach size value does not effect the train time.

In the nvtop image above, we can find four peaks of GPU load, which represents four epochs, the first one is the first learning of `vision_learner`, and the next three is the fine tune learning. The mem of GPU does not rise. So we do not use much mem of GPU in the train.

